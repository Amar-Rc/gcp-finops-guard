
# GCP Serverless FinOps Engine ðŸ›¡ï¸

A serverless, event-driven framework to monitor BigQuery and Snowflake costs in real-time. This engine proactively identifies expensive queries and idle resources, alerting engineering teams via Slack to prevent billing shock.

ðŸ— Architecture

The solution uses a serverless architecture to minimize maintenance overhead. It runs entirely within the GCP Free Tier limits.

`graph LR
    A[Cloud Scheduler] -- "Trigger (Every 8 AM)" --> B(Cloud Functions Gen2)
    B -- "Fetch Creds" --> C{Secret Manager}
    B -- "Scan Metadata" --> D[(BigQuery INFO_SCHEMA)]
    B -- "Scan Metadata" --> E[(Snowflake ACCOUNT_USAGE)]
    B -- "If Anomaly Detected" --> F[Slack API]
    F --> G[Alert Engineering Team]`


ðŸš€ Key Features

Multi-Cloud Monitoring: Scans both BigQuery (via INFORMATION_SCHEMA.JOBS) and Snowflake (via ACCOUNT_USAGE).

Zero-Maintenance Compute: Runs on Google Cloud Functions (Gen 2). No EC2/VM management required.

Security First: Uses GCP Secret Manager to handle database credentials and API hooks. No hardcoded secrets.

Cost: ~$0.00/month (Uses minimal compute seconds and BigQuery metadata is free to query).

ðŸ“¸ Example Alert

When a query exceeds the defined threshold (e.g., > $5.00 or > 1TB scanned), the team receives an immediate notification:

(Note: Add your screenshot here. Create a folder named 'images' in your repo)

ðŸ› ï¸ Tech Stack

Cloud Provider: Google Cloud Platform (GCP)

Compute: Cloud Functions (Python 3.10 runtime)

Orchestration: Cloud Scheduler

Infrastructure as Code: gcloud CLI / Terraform (planned)

Security: IAM Service Accounts, Secret Manager

Warehouses: BigQuery, Snowflake

âš™ï¸ Setup & Deployment

1. Prerequisites

GCP Project with billing enabled.

gcloud CLI installed and authenticated.

Slack Incoming Webhook URL.

2. Environment Variables & Secrets

We use Secret Manager for production security.

# Create secrets in GCP
gcloud secrets create slack_webhook --data-file=./slack_url.txt
gcloud secrets create snowflake_password --data-file=./pwd.txt


3. Local Development

# Clone repo
git clone [https://github.com/Amar-Rc/gcp-finops-guard.git](https://github.com/Amar-Rc/gcp-finops-guard.git)
cd gcp-finops-guard

# Install dependencies
pip install -r requirements.txt

# Run locally (simulated)
python main.py


4. Deploy to GCP

Deploy using the standard gcloud command. We use a dedicated Service Account (sa-finops-runner) to enforce Least Privilege.

gcloud functions deploy finops-cost-monitor \
    --gen2 \
    --runtime=python310 \
    --region=us-central1 \
    --source=. \
    --entry-point=check_bigquery_costs \
    --trigger-http \
    --service-account=sa-finops-runner@finops-monitor-prod.iam.gserviceaccount.com


ðŸ§  Logic & Thresholds

The core logic resides in main.py. It calculates cost based on bytes billed:

BigQuery Logic:

SELECT 
    user_email, 
    job_id,
    total_bytes_billed, 
    (total_bytes_billed / 1099511627776) * 5.00 AS cost_usd
FROM region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
AND total_bytes_billed > 214748364800 -- Alert if > 200GB scanned
ORDER BY cost_usd DESC


ðŸ“ˆ Impact

15% Reduction in monthly cloud spend by identifying "Zombie" scheduled queries.

Reduced MTTR (Mean Time To Resolution) for bad queries from days to minutes.

Eliminated manual "Cost Analysis" spreadsheet tasks.
